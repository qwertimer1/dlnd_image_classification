{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [01:39, 1.71MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6349146f98>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    a= 0\n",
    "    b= 1\n",
    "    Xmin = 0\n",
    "    Xmax = 255\n",
    "    \n",
    "    X_ = a + ((x - Xmin)*(b-a))/(Xmax-Xmin)\n",
    "    \n",
    "    \n",
    "    return X_\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    \n",
    "    return np.eye(10)[x]\n",
    "    \"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    "If you're finding it hard to dedicate enough time for this course a week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) to build each layer, except \"Convolutional & Max Pooling\" layer.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    "If you would like to get the most of this course, try to solve all the problems without TF Layers.  Let's begin!\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape = (None,image_shape[0],image_shape[1],image_shape[2]), name = 'x')\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    y = tf.placeholder(tf.float32, shape = (None, n_classes), name = 'y')\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    return keep_prob\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "Note: You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.  You're free to use any TensorFlow package for all the other layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 (4, 4) (2, 2) (2, 2)\n",
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    print(conv_num_outputs, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    weights = tf.Variable(tf.truncated_normal(\n",
    "        [conv_ksize[0],\n",
    "         conv_ksize[1],\n",
    "         x_tensor.get_shape().as_list()[3],\n",
    "         conv_num_outputs],                       #Weights are initialised with tf.Variable to a truncated normal with \n",
    "                                                 #shape =[height, width, depth, output depth]\n",
    "         stddev = 0.05\n",
    "    ))\n",
    "    \n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "\n",
    "\n",
    "    conv_layer = tf.nn.conv2d(\n",
    "        input = x_tensor,\n",
    "        filter = weights,\n",
    "        strides = [1, conv_strides[0], conv_strides[1], 1],\n",
    "        padding = 'SAME',\n",
    "        name = 'cl1') \n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    conv_layer = tf.nn.max_pool(\n",
    "        value = conv_layer,\n",
    "        ksize = [1, pool_ksize[0], pool_ksize[1], 1],\n",
    "        strides = [1, pool_strides[0], pool_strides[1], 1],\n",
    "        padding = 'SAME')\n",
    "    return conv_layer \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    result = tf.contrib.layers.flatten(x_tensor)\n",
    "    return result\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "\n",
    "\n",
    "    return tf.contrib.layers.fully_connected(x_tensor,num_outputs) \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). You can use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for this layer.\n",
    "\n",
    "Note: Activation, softmax, or cross entropy shouldn't be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    return tf.contrib.layers.fully_connected(x_tensor,num_outputs) \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 (1, 1) (4, 4) (2, 2)\n",
      "128 (1, 1) (4, 4) (2, 2)\n",
      "256 (1, 1) (4, 4) (2, 2)\n",
      "32 (1, 1) (4, 4) (2, 2)\n",
      "128 (1, 1) (4, 4) (2, 2)\n",
      "256 (1, 1) (4, 4) (2, 2)\n",
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    \n",
    "    conv_num_outputs = [32, 128, 256]\n",
    "    conv_ksize = [(4, 4),(6,6), (8,8)]\n",
    "    conv_strides = (1, 1)\n",
    "    pool_ksize = (4, 4)\n",
    "    pool_strides = (2, 2)\n",
    "    num_outputs = 10\n",
    "    \n",
    "    \n",
    "    conv1 = conv2d_maxpool(x, conv_num_outputs[0], conv_ksize[0], conv_strides, pool_ksize, pool_strides)\n",
    "    drop1 = tf.nn.dropout(conv1, keep_prob=keep_prob)\n",
    "    conv2 = conv2d_maxpool(drop1, conv_num_outputs[1], conv_ksize[1], conv_strides, pool_ksize, pool_strides)\n",
    "    drop2 = tf.nn.dropout(conv2, keep_prob=keep_prob)\n",
    "    conv3 = conv2d_maxpool(drop2, conv_num_outputs[2], conv_ksize[2], conv_strides, pool_ksize, pool_strides)\n",
    "    drop3 = tf.nn.dropout(conv3, keep_prob=keep_prob)\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    flatten1 = flatten(drop3)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    fc1 = fully_conn(flatten1, num_outputs)\n",
    "    fc2 = fully_conn(fc1, num_outputs)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out1 = output(fc2, num_outputs)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return out1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y:label_batch, keep_prob: keep_probability})\n",
    "    pass\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0 })\n",
    "    valid_accur = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    print(\"loss: {}\".format(loss))\n",
    "    print(\"accuracy: {}\".format(valid_accur))\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 128\n",
    "keep_probability = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "WARNING:tensorflow:From <ipython-input-18-dcae40b0160b>:7 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.2681620121002197\n",
      "accuracy: 0.1605999916791916\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 2.2412025928497314\n",
      "accuracy: 0.19380000233650208\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 2.2350850105285645\n",
      "accuracy: 0.16940000653266907\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 2.186922550201416\n",
      "accuracy: 0.1881999969482422\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 2.1526546478271484\n",
      "accuracy: 0.1937999725341797\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 2.147763252258301\n",
      "accuracy: 0.1881999969482422\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 2.132171630859375\n",
      "accuracy: 0.19099998474121094\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 2.1124229431152344\n",
      "accuracy: 0.19619999825954437\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 2.0969018936157227\n",
      "accuracy: 0.20459997653961182\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 2.0560550689697266\n",
      "accuracy: 0.26759999990463257\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 2.012007713317871\n",
      "accuracy: 0.3009999990463257\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 1.8103222846984863\n",
      "accuracy: 0.33459997177124023\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 1.7532670497894287\n",
      "accuracy: 0.3285999596118927\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 1.689798355102539\n",
      "accuracy: 0.3351999819278717\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 1.6147809028625488\n",
      "accuracy: 0.3479999899864197\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 1.5685198307037354\n",
      "accuracy: 0.3587999939918518\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 1.5218693017959595\n",
      "accuracy: 0.3765999972820282\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 1.4438190460205078\n",
      "accuracy: 0.3651999533176422\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 1.3400723934173584\n",
      "accuracy: 0.40639999508857727\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 1.3206162452697754\n",
      "accuracy: 0.3905999958515167\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 1.2041840553283691\n",
      "accuracy: 0.4163999557495117\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 1.1617989540100098\n",
      "accuracy: 0.41579997539520264\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 1.0931644439697266\n",
      "accuracy: 0.4471999406814575\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 0.9816800355911255\n",
      "accuracy: 0.4559999406337738\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 0.8937771916389465\n",
      "accuracy: 0.4689999520778656\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 0.7824254035949707\n",
      "accuracy: 0.4813999533653259\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 0.6644235253334045\n",
      "accuracy: 0.4835999310016632\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 0.6670512557029724\n",
      "accuracy: 0.4971999526023865\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 0.6110001802444458\n",
      "accuracy: 0.5013999938964844\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 0.5612636208534241\n",
      "accuracy: 0.504599928855896\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 0.5423604249954224\n",
      "accuracy: 0.5119999051094055\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 0.5106984972953796\n",
      "accuracy: 0.5157999396324158\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 0.5234546661376953\n",
      "accuracy: 0.5077999830245972\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 0.4779789447784424\n",
      "accuracy: 0.5339999198913574\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 0.43346482515335083\n",
      "accuracy: 0.5367999076843262\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 0.4733620285987854\n",
      "accuracy: 0.5389999151229858\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 0.5339593887329102\n",
      "accuracy: 0.536799967288971\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 0.32181403040885925\n",
      "accuracy: 0.6053999066352844\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 0.2906795144081116\n",
      "accuracy: 0.6065998673439026\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 0.2211652398109436\n",
      "accuracy: 0.6029999256134033\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 0.27133068442344666\n",
      "accuracy: 0.6225998997688293\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 0.3367483615875244\n",
      "accuracy: 0.6119999885559082\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 0.27519646286964417\n",
      "accuracy: 0.6147998571395874\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 0.14353029429912567\n",
      "accuracy: 0.608799934387207\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 0.14300447702407837\n",
      "accuracy: 0.6123999357223511\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 0.3791033625602722\n",
      "accuracy: 0.6169999241828918\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 0.08225007355213165\n",
      "accuracy: 0.6133999824523926\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 0.23608464002609253\n",
      "accuracy: 0.6177998781204224\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 0.09093131124973297\n",
      "accuracy: 0.6243999004364014\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 0.07527623325586319\n",
      "accuracy: 0.6141998767852783\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 0.07696593552827835\n",
      "accuracy: 0.6283999085426331\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 0.22548454999923706\n",
      "accuracy: 0.625999927520752\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 0.16319164633750916\n",
      "accuracy: 0.6171998977661133\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 0.23676881194114685\n",
      "accuracy: 0.6211998462677002\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 0.09472661465406418\n",
      "accuracy: 0.6129999160766602\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 0.07426704466342926\n",
      "accuracy: 0.6171998977661133\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 0.0686502531170845\n",
      "accuracy: 0.6245998740196228\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 0.058542560786008835\n",
      "accuracy: 0.6145999431610107\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 0.3407564163208008\n",
      "accuracy: 0.6173998713493347\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 0.062266282737255096\n",
      "accuracy: 0.6225999593734741\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 0.23804065585136414\n",
      "accuracy: 0.6053999066352844\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 0.05823970586061478\n",
      "accuracy: 0.6115999817848206\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 0.06145549193024635\n",
      "accuracy: 0.6057999134063721\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 0.11840781569480896\n",
      "accuracy: 0.6183999180793762\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 0.14497874677181244\n",
      "accuracy: 0.616599977016449\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 0.062085457146167755\n",
      "accuracy: 0.6217999458312988\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 0.061521559953689575\n",
      "accuracy: 0.6181999444961548\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 0.058198317885398865\n",
      "accuracy: 0.6171998977661133\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 0.05621028691530228\n",
      "accuracy: 0.611799955368042\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 0.10568371415138245\n",
      "accuracy: 0.6267998814582825\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 0.06156556308269501\n",
      "accuracy: 0.6245999336242676\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 0.05484788864850998\n",
      "accuracy: 0.6327998638153076\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 0.05415855720639229\n",
      "accuracy: 0.6097999215126038\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 0.057554423809051514\n",
      "accuracy: 0.6265999674797058\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 0.05728611350059509\n",
      "accuracy: 0.6375999450683594\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 0.058539677411317825\n",
      "accuracy: 0.6107999086380005\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 0.05615156143903732\n",
      "accuracy: 0.6275998950004578\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 0.05523690953850746\n",
      "accuracy: 0.6175999641418457\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 0.05788755044341087\n",
      "accuracy: 0.6273999214172363\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 0.055091921240091324\n",
      "accuracy: 0.6263999342918396\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 0.05563153699040413\n",
      "accuracy: 0.6405999064445496\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 0.06028727814555168\n",
      "accuracy: 0.6361998915672302\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 0.10720060765743256\n",
      "accuracy: 0.6327998638153076\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 0.0543125718832016\n",
      "accuracy: 0.6307998895645142\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 0.05470185726881027\n",
      "accuracy: 0.6241998672485352\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 0.054967813193798065\n",
      "accuracy: 0.6223998665809631\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 0.05496795102953911\n",
      "accuracy: 0.6293998956680298\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 0.053935255855321884\n",
      "accuracy: 0.6359999179840088\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 0.05434815213084221\n",
      "accuracy: 0.6255999207496643\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 0.056668464094400406\n",
      "accuracy: 0.621799886226654\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 0.05433472618460655\n",
      "accuracy: 0.6099998950958252\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 0.05455750972032547\n",
      "accuracy: 0.6263998746871948\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 0.05446664243936539\n",
      "accuracy: 0.6319998502731323\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 0.05476299673318863\n",
      "accuracy: 0.6197998523712158\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 0.05448111519217491\n",
      "accuracy: 0.6301999092102051\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 0.05355558544397354\n",
      "accuracy: 0.6235998868942261\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 0.05359920114278793\n",
      "accuracy: 0.6237999200820923\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 0.05400940775871277\n",
      "accuracy: 0.6279999017715454\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 0.05456653609871864\n",
      "accuracy: 0.624799907207489\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 0.053836122155189514\n",
      "accuracy: 0.6321998834609985\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "WARNING:tensorflow:From <ipython-input-19-95f0569176bb>:9 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Epoch  1, CIFAR-10 Batch 1:  loss: 2.2913219928741455\n",
      "accuracy: 0.11339998990297318\n",
      "Epoch  1, CIFAR-10 Batch 2:  loss: 2.3126182556152344\n",
      "accuracy: 0.1817999929189682\n",
      "Epoch  1, CIFAR-10 Batch 3:  loss: 2.1440234184265137\n",
      "accuracy: 0.20019999146461487\n",
      "Epoch  1, CIFAR-10 Batch 4:  loss: 2.2397968769073486\n",
      "accuracy: 0.1841999888420105\n",
      "Epoch  1, CIFAR-10 Batch 5:  loss: 2.1565935611724854\n",
      "accuracy: 0.24779999256134033\n",
      "Epoch  2, CIFAR-10 Batch 1:  loss: 1.9136962890625\n",
      "accuracy: 0.2985999882221222\n",
      "Epoch  2, CIFAR-10 Batch 2:  loss: 2.099240779876709\n",
      "accuracy: 0.3351999819278717\n",
      "Epoch  2, CIFAR-10 Batch 3:  loss: 1.864130973815918\n",
      "accuracy: 0.34219998121261597\n",
      "Epoch  2, CIFAR-10 Batch 4:  loss: 1.928680658340454\n",
      "accuracy: 0.3649999797344208\n",
      "Epoch  2, CIFAR-10 Batch 5:  loss: 1.7620887756347656\n",
      "accuracy: 0.37519997358322144\n",
      "Epoch  3, CIFAR-10 Batch 1:  loss: 1.7336084842681885\n",
      "accuracy: 0.38599997758865356\n",
      "Epoch  3, CIFAR-10 Batch 2:  loss: 1.8795530796051025\n",
      "accuracy: 0.39139991998672485\n",
      "Epoch  3, CIFAR-10 Batch 3:  loss: 1.7459170818328857\n",
      "accuracy: 0.3823999762535095\n",
      "Epoch  3, CIFAR-10 Batch 4:  loss: 1.6447408199310303\n",
      "accuracy: 0.41280001401901245\n",
      "Epoch  3, CIFAR-10 Batch 5:  loss: 1.49733567237854\n",
      "accuracy: 0.4453999698162079\n",
      "Epoch  4, CIFAR-10 Batch 1:  loss: 1.6751405000686646\n",
      "accuracy: 0.4461999833583832\n",
      "Epoch  4, CIFAR-10 Batch 2:  loss: 1.565314531326294\n",
      "accuracy: 0.4719999432563782\n",
      "Epoch  4, CIFAR-10 Batch 3:  loss: 1.4850724935531616\n",
      "accuracy: 0.4813999533653259\n",
      "Epoch  4, CIFAR-10 Batch 4:  loss: 1.4231340885162354\n",
      "accuracy: 0.5189999341964722\n",
      "Epoch  4, CIFAR-10 Batch 5:  loss: 1.2756078243255615\n",
      "accuracy: 0.5215999484062195\n",
      "Epoch  5, CIFAR-10 Batch 1:  loss: 1.3643016815185547\n",
      "accuracy: 0.5421999096870422\n",
      "Epoch  5, CIFAR-10 Batch 2:  loss: 1.4379222393035889\n",
      "accuracy: 0.5447999238967896\n",
      "Epoch  5, CIFAR-10 Batch 3:  loss: 1.1869062185287476\n",
      "accuracy: 0.5605999231338501\n",
      "Epoch  5, CIFAR-10 Batch 4:  loss: 1.0823850631713867\n",
      "accuracy: 0.5631999373435974\n",
      "Epoch  5, CIFAR-10 Batch 5:  loss: 0.9728004932403564\n",
      "accuracy: 0.5829999446868896\n",
      "Epoch  6, CIFAR-10 Batch 1:  loss: 1.234360694885254\n",
      "accuracy: 0.5995998978614807\n",
      "Epoch  6, CIFAR-10 Batch 2:  loss: 1.0842053890228271\n",
      "accuracy: 0.6233999133110046\n",
      "Epoch  6, CIFAR-10 Batch 3:  loss: 0.8751484155654907\n",
      "accuracy: 0.6171998977661133\n",
      "Epoch  6, CIFAR-10 Batch 4:  loss: 0.935920238494873\n",
      "accuracy: 0.6287999153137207\n",
      "Epoch  6, CIFAR-10 Batch 5:  loss: 0.9047718048095703\n",
      "accuracy: 0.6579998731613159\n",
      "Epoch  7, CIFAR-10 Batch 1:  loss: 0.9372885823249817\n",
      "accuracy: 0.6429998874664307\n",
      "Epoch  7, CIFAR-10 Batch 2:  loss: 0.8428856730461121\n",
      "accuracy: 0.6547999382019043\n",
      "Epoch  7, CIFAR-10 Batch 3:  loss: 0.5709102749824524\n",
      "accuracy: 0.6581999063491821\n",
      "Epoch  7, CIFAR-10 Batch 4:  loss: 0.8299260139465332\n",
      "accuracy: 0.6511999368667603\n",
      "Epoch  7, CIFAR-10 Batch 5:  loss: 0.6375606060028076\n",
      "accuracy: 0.6501998901367188\n",
      "Epoch  8, CIFAR-10 Batch 1:  loss: 0.7772104740142822\n",
      "accuracy: 0.6639999151229858\n",
      "Epoch  8, CIFAR-10 Batch 2:  loss: 0.6858783960342407\n",
      "accuracy: 0.6743999123573303\n",
      "Epoch  8, CIFAR-10 Batch 3:  loss: 0.5071824789047241\n",
      "accuracy: 0.6575998663902283\n",
      "Epoch  8, CIFAR-10 Batch 4:  loss: 0.7726267576217651\n",
      "accuracy: 0.6701998710632324\n",
      "Epoch  8, CIFAR-10 Batch 5:  loss: 0.5232235789299011\n",
      "accuracy: 0.6687998175621033\n",
      "Epoch  9, CIFAR-10 Batch 1:  loss: 0.7680301666259766\n",
      "accuracy: 0.6711998581886292\n",
      "Epoch  9, CIFAR-10 Batch 2:  loss: 0.5945495367050171\n",
      "accuracy: 0.6811999082565308\n",
      "Epoch  9, CIFAR-10 Batch 3:  loss: 0.4497528076171875\n",
      "accuracy: 0.6867998838424683\n",
      "Epoch  9, CIFAR-10 Batch 4:  loss: 0.7267041206359863\n",
      "accuracy: 0.6739998459815979\n",
      "Epoch  9, CIFAR-10 Batch 5:  loss: 0.4879603981971741\n",
      "accuracy: 0.6891998648643494\n",
      "Epoch 10, CIFAR-10 Batch 1:  loss: 0.6480220556259155\n",
      "accuracy: 0.6929998397827148\n",
      "Epoch 10, CIFAR-10 Batch 2:  loss: 0.5833429098129272\n",
      "accuracy: 0.6985998749732971\n",
      "Epoch 10, CIFAR-10 Batch 3:  loss: 0.3998670279979706\n",
      "accuracy: 0.6899998784065247\n",
      "Epoch 10, CIFAR-10 Batch 4:  loss: 0.6844538450241089\n",
      "accuracy: 0.6867998838424683\n",
      "Epoch 10, CIFAR-10 Batch 5:  loss: 0.39871037006378174\n",
      "accuracy: 0.6881998181343079\n",
      "Epoch 11, CIFAR-10 Batch 1:  loss: 0.5692171454429626\n",
      "accuracy: 0.700999915599823\n",
      "Epoch 11, CIFAR-10 Batch 2:  loss: 0.5861304998397827\n",
      "accuracy: 0.7005997896194458\n",
      "Epoch 11, CIFAR-10 Batch 3:  loss: 0.4023979604244232\n",
      "accuracy: 0.6807998418807983\n",
      "Epoch 11, CIFAR-10 Batch 4:  loss: 0.6514474153518677\n",
      "accuracy: 0.6829999089241028\n",
      "Epoch 11, CIFAR-10 Batch 5:  loss: 0.4114611744880676\n",
      "accuracy: 0.6885998845100403\n",
      "Epoch 12, CIFAR-10 Batch 1:  loss: 0.5264189839363098\n",
      "accuracy: 0.6927998661994934\n",
      "Epoch 12, CIFAR-10 Batch 2:  loss: 0.5297752022743225\n",
      "accuracy: 0.6869998574256897\n",
      "Epoch 12, CIFAR-10 Batch 3:  loss: 0.37744954228401184\n",
      "accuracy: 0.6853998899459839\n",
      "Epoch 12, CIFAR-10 Batch 4:  loss: 0.6041000485420227\n",
      "accuracy: 0.6921998858451843\n",
      "Epoch 12, CIFAR-10 Batch 5:  loss: 0.35164254903793335\n",
      "accuracy: 0.6935998797416687\n",
      "Epoch 13, CIFAR-10 Batch 1:  loss: 0.4996383786201477\n",
      "accuracy: 0.696199893951416\n",
      "Epoch 13, CIFAR-10 Batch 2:  loss: 0.5035688281059265\n",
      "accuracy: 0.6897998452186584\n",
      "Epoch 13, CIFAR-10 Batch 3:  loss: 0.3947247862815857\n",
      "accuracy: 0.6801998615264893\n",
      "Epoch 13, CIFAR-10 Batch 4:  loss: 0.6321530938148499\n",
      "accuracy: 0.689599871635437\n",
      "Epoch 13, CIFAR-10 Batch 5:  loss: 0.3437124788761139\n",
      "accuracy: 0.7015999555587769\n",
      "Epoch 14, CIFAR-10 Batch 1:  loss: 0.44588330388069153\n",
      "accuracy: 0.6899998188018799\n",
      "Epoch 14, CIFAR-10 Batch 2:  loss: 0.5126092433929443\n",
      "accuracy: 0.6889998912811279\n",
      "Epoch 14, CIFAR-10 Batch 3:  loss: 0.37020206451416016\n",
      "accuracy: 0.700799822807312\n",
      "Epoch 14, CIFAR-10 Batch 4:  loss: 0.6053801774978638\n",
      "accuracy: 0.6989998817443848\n",
      "Epoch 14, CIFAR-10 Batch 5:  loss: 0.3062642812728882\n",
      "accuracy: 0.703799843788147\n",
      "Epoch 15, CIFAR-10 Batch 1:  loss: 0.4620833396911621\n",
      "accuracy: 0.6953998804092407\n",
      "Epoch 15, CIFAR-10 Batch 2:  loss: 0.49861782789230347\n",
      "accuracy: 0.7005998492240906\n",
      "Epoch 15, CIFAR-10 Batch 3:  loss: 0.37749889492988586\n",
      "accuracy: 0.6923999190330505\n",
      "Epoch 15, CIFAR-10 Batch 4:  loss: 0.581436038017273\n",
      "accuracy: 0.6989998817443848\n",
      "Epoch 15, CIFAR-10 Batch 5:  loss: 0.347412645816803\n",
      "accuracy: 0.6903998255729675\n",
      "Epoch 16, CIFAR-10 Batch 1:  loss: 0.3779900372028351\n",
      "accuracy: 0.6943998336791992\n",
      "Epoch 16, CIFAR-10 Batch 2:  loss: 0.44112783670425415\n",
      "accuracy: 0.6929998397827148\n",
      "Epoch 16, CIFAR-10 Batch 3:  loss: 0.34103015065193176\n",
      "accuracy: 0.7059998512268066\n",
      "Epoch 16, CIFAR-10 Batch 4:  loss: 0.5495803356170654\n",
      "accuracy: 0.7021998167037964\n",
      "Epoch 16, CIFAR-10 Batch 5:  loss: 0.2849803864955902\n",
      "accuracy: 0.7013998627662659\n",
      "Epoch 17, CIFAR-10 Batch 1:  loss: 0.3606318235397339\n",
      "accuracy: 0.7001998424530029\n",
      "Epoch 17, CIFAR-10 Batch 2:  loss: 0.4264453649520874\n",
      "accuracy: 0.7013998031616211\n",
      "Epoch 17, CIFAR-10 Batch 3:  loss: 0.32442787289619446\n",
      "accuracy: 0.7043998837471008\n",
      "Epoch 17, CIFAR-10 Batch 4:  loss: 0.3753734230995178\n",
      "accuracy: 0.7125998735427856\n",
      "Epoch 17, CIFAR-10 Batch 5:  loss: 0.13374179601669312\n",
      "accuracy: 0.7243998646736145\n",
      "Epoch 18, CIFAR-10 Batch 1:  loss: 0.1121654361486435\n",
      "accuracy: 0.7311998605728149\n",
      "Epoch 18, CIFAR-10 Batch 2:  loss: 0.2884520888328552\n",
      "accuracy: 0.7229998707771301\n",
      "Epoch 18, CIFAR-10 Batch 3:  loss: 0.12472069263458252\n",
      "accuracy: 0.741399884223938\n",
      "Epoch 18, CIFAR-10 Batch 4:  loss: 0.21613913774490356\n",
      "accuracy: 0.7421998977661133\n",
      "Epoch 18, CIFAR-10 Batch 5:  loss: 0.09548327326774597\n",
      "accuracy: 0.7241998314857483\n",
      "Epoch 19, CIFAR-10 Batch 1:  loss: 0.1076241210103035\n",
      "accuracy: 0.729999840259552\n",
      "Epoch 19, CIFAR-10 Batch 2:  loss: 0.19079765677452087\n",
      "accuracy: 0.7355998754501343\n",
      "Epoch 19, CIFAR-10 Batch 3:  loss: 0.09207326173782349\n",
      "accuracy: 0.7003999352455139\n",
      "Epoch 19, CIFAR-10 Batch 4:  loss: 0.2054009735584259\n",
      "accuracy: 0.7331998348236084\n",
      "Epoch 19, CIFAR-10 Batch 5:  loss: 0.10031750798225403\n",
      "accuracy: 0.7399998307228088\n",
      "Epoch 20, CIFAR-10 Batch 1:  loss: 0.04543525353074074\n",
      "accuracy: 0.745999813079834\n",
      "Epoch 20, CIFAR-10 Batch 2:  loss: 0.12546944618225098\n",
      "accuracy: 0.7387998104095459\n",
      "Epoch 20, CIFAR-10 Batch 3:  loss: 0.05085332319140434\n",
      "accuracy: 0.7331998348236084\n",
      "Epoch 20, CIFAR-10 Batch 4:  loss: 0.19539153575897217\n",
      "accuracy: 0.7323998212814331\n",
      "Epoch 20, CIFAR-10 Batch 5:  loss: 0.05031587556004524\n",
      "accuracy: 0.7337997555732727\n",
      "Epoch 21, CIFAR-10 Batch 1:  loss: 0.048319362103939056\n",
      "accuracy: 0.7361998558044434\n",
      "Epoch 21, CIFAR-10 Batch 2:  loss: 0.1665721833705902\n",
      "accuracy: 0.7373998165130615\n",
      "Epoch 21, CIFAR-10 Batch 3:  loss: 0.03933686763048172\n",
      "accuracy: 0.7459998726844788\n",
      "Epoch 21, CIFAR-10 Batch 4:  loss: 0.11977224051952362\n",
      "accuracy: 0.7309998273849487\n",
      "Epoch 21, CIFAR-10 Batch 5:  loss: 0.0764952078461647\n",
      "accuracy: 0.7417998313903809\n",
      "Epoch 22, CIFAR-10 Batch 1:  loss: 0.10860013216733932\n",
      "accuracy: 0.734799861907959\n",
      "Epoch 22, CIFAR-10 Batch 2:  loss: 0.10374528914690018\n",
      "accuracy: 0.7455998063087463\n",
      "Epoch 22, CIFAR-10 Batch 3:  loss: 0.022259557619690895\n",
      "accuracy: 0.7389998435974121\n",
      "Epoch 22, CIFAR-10 Batch 4:  loss: 0.11083328723907471\n",
      "accuracy: 0.7371999025344849\n",
      "Epoch 22, CIFAR-10 Batch 5:  loss: 0.020011398941278458\n",
      "accuracy: 0.7527998685836792\n",
      "Epoch 23, CIFAR-10 Batch 1:  loss: 0.032267455011606216\n",
      "accuracy: 0.7369998693466187\n",
      "Epoch 23, CIFAR-10 Batch 2:  loss: 0.08914970606565475\n",
      "accuracy: 0.7293999195098877\n",
      "Epoch 23, CIFAR-10 Batch 3:  loss: 0.03200828284025192\n",
      "accuracy: 0.7449997663497925\n",
      "Epoch 23, CIFAR-10 Batch 4:  loss: 0.023463670164346695\n",
      "accuracy: 0.7439998388290405\n",
      "Epoch 23, CIFAR-10 Batch 5:  loss: 0.048587605357170105\n",
      "accuracy: 0.740199863910675\n",
      "Epoch 24, CIFAR-10 Batch 1:  loss: 0.03734011575579643\n",
      "accuracy: 0.7381998300552368\n",
      "Epoch 24, CIFAR-10 Batch 2:  loss: 0.05836911126971245\n",
      "accuracy: 0.7179998159408569\n",
      "Epoch 24, CIFAR-10 Batch 3:  loss: 0.018339883536100388\n",
      "accuracy: 0.7519998550415039\n",
      "Epoch 24, CIFAR-10 Batch 4:  loss: 0.03988485783338547\n",
      "accuracy: 0.7397997975349426\n",
      "Epoch 24, CIFAR-10 Batch 5:  loss: 0.034651897847652435\n",
      "accuracy: 0.7545998692512512\n",
      "Epoch 25, CIFAR-10 Batch 1:  loss: 0.032705795019865036\n",
      "accuracy: 0.7421998977661133\n",
      "Epoch 25, CIFAR-10 Batch 2:  loss: 0.06772048771381378\n",
      "accuracy: 0.731799840927124\n",
      "Epoch 25, CIFAR-10 Batch 3:  loss: 0.014058533124625683\n",
      "accuracy: 0.7457998394966125\n",
      "Epoch 25, CIFAR-10 Batch 4:  loss: 0.02754502184689045\n",
      "accuracy: 0.7415998578071594\n",
      "Epoch 25, CIFAR-10 Batch 5:  loss: 0.04606475308537483\n",
      "accuracy: 0.7475999593734741\n",
      "Epoch 26, CIFAR-10 Batch 1:  loss: 0.013243447989225388\n",
      "accuracy: 0.7441999316215515\n",
      "Epoch 26, CIFAR-10 Batch 2:  loss: 0.04440624266862869\n",
      "accuracy: 0.7279999256134033\n",
      "Epoch 26, CIFAR-10 Batch 3:  loss: 0.007628960069268942\n",
      "accuracy: 0.7437998056411743\n",
      "Epoch 26, CIFAR-10 Batch 4:  loss: 0.03890907019376755\n",
      "accuracy: 0.7487998604774475\n",
      "Epoch 26, CIFAR-10 Batch 5:  loss: 0.05050230026245117\n",
      "accuracy: 0.7575998306274414\n",
      "Epoch 27, CIFAR-10 Batch 1:  loss: 0.021503880620002747\n",
      "accuracy: 0.72819983959198\n",
      "Epoch 27, CIFAR-10 Batch 2:  loss: 0.025000063702464104\n",
      "accuracy: 0.7335997819900513\n",
      "Epoch 27, CIFAR-10 Batch 3:  loss: 0.008170362561941147\n",
      "accuracy: 0.7453998327255249\n",
      "Epoch 27, CIFAR-10 Batch 4:  loss: 0.06896471232175827\n",
      "accuracy: 0.7389997839927673\n",
      "Epoch 27, CIFAR-10 Batch 5:  loss: 0.013225821778178215\n",
      "accuracy: 0.7509998679161072\n",
      "Epoch 28, CIFAR-10 Batch 1:  loss: 0.007606033235788345\n",
      "accuracy: 0.7465997934341431\n",
      "Epoch 28, CIFAR-10 Batch 2:  loss: 0.016510682180523872\n",
      "accuracy: 0.726399838924408\n",
      "Epoch 28, CIFAR-10 Batch 3:  loss: 0.011814139783382416\n",
      "accuracy: 0.7501997947692871\n",
      "Epoch 28, CIFAR-10 Batch 4:  loss: 0.019144834950566292\n",
      "accuracy: 0.752599835395813\n",
      "Epoch 28, CIFAR-10 Batch 5:  loss: 0.013020908460021019\n",
      "accuracy: 0.7483997941017151\n",
      "Epoch 29, CIFAR-10 Batch 1:  loss: 0.017090968787670135\n",
      "accuracy: 0.7479997873306274\n",
      "Epoch 29, CIFAR-10 Batch 2:  loss: 0.016083460301160812\n",
      "accuracy: 0.7479998469352722\n",
      "Epoch 29, CIFAR-10 Batch 3:  loss: 0.003962330520153046\n",
      "accuracy: 0.7415998578071594\n",
      "Epoch 29, CIFAR-10 Batch 4:  loss: 0.00886077806353569\n",
      "accuracy: 0.7419998645782471\n",
      "Epoch 29, CIFAR-10 Batch 5:  loss: 0.007069719024002552\n",
      "accuracy: 0.7383999228477478\n",
      "Epoch 30, CIFAR-10 Batch 1:  loss: 0.014045343734323978\n",
      "accuracy: 0.7337998747825623\n",
      "Epoch 30, CIFAR-10 Batch 2:  loss: 0.009599795565009117\n",
      "accuracy: 0.7293998599052429\n",
      "Epoch 30, CIFAR-10 Batch 3:  loss: 0.0016631314065307379\n",
      "accuracy: 0.7479997873306274\n",
      "Epoch 30, CIFAR-10 Batch 4:  loss: 0.013618888333439827\n",
      "accuracy: 0.7419998645782471\n",
      "Epoch 30, CIFAR-10 Batch 5:  loss: 0.00686545530334115\n",
      "accuracy: 0.7375998497009277\n",
      "Epoch 31, CIFAR-10 Batch 1:  loss: 0.014620491303503513\n",
      "accuracy: 0.7481998205184937\n",
      "Epoch 31, CIFAR-10 Batch 2:  loss: 0.005399397108703852\n",
      "accuracy: 0.7421998977661133\n",
      "Epoch 31, CIFAR-10 Batch 3:  loss: 0.005818595644086599\n",
      "accuracy: 0.7423998713493347\n",
      "Epoch 31, CIFAR-10 Batch 4:  loss: 0.042770035564899445\n",
      "accuracy: 0.7421998977661133\n",
      "Epoch 31, CIFAR-10 Batch 5:  loss: 0.003492948831990361\n",
      "accuracy: 0.7513998746871948\n",
      "Epoch 32, CIFAR-10 Batch 1:  loss: 0.005256536416709423\n",
      "accuracy: 0.7477998733520508\n",
      "Epoch 32, CIFAR-10 Batch 2:  loss: 0.011269939132034779\n",
      "accuracy: 0.7499998211860657\n",
      "Epoch 32, CIFAR-10 Batch 3:  loss: 0.00385289266705513\n",
      "accuracy: 0.7529998421669006\n",
      "Epoch 32, CIFAR-10 Batch 4:  loss: 0.011431768536567688\n",
      "accuracy: 0.7425998449325562\n",
      "Epoch 32, CIFAR-10 Batch 5:  loss: 0.007590617518872023\n",
      "accuracy: 0.7467997670173645\n",
      "Epoch 33, CIFAR-10 Batch 1:  loss: 0.0035277227871119976\n",
      "accuracy: 0.7487998008728027\n",
      "Epoch 33, CIFAR-10 Batch 2:  loss: 0.0024429652839899063\n",
      "accuracy: 0.7505998015403748\n",
      "Epoch 33, CIFAR-10 Batch 3:  loss: 0.010338440537452698\n",
      "accuracy: 0.7433998584747314\n",
      "Epoch 33, CIFAR-10 Batch 4:  loss: 0.002860268112272024\n",
      "accuracy: 0.7463998794555664\n",
      "Epoch 33, CIFAR-10 Batch 5:  loss: 0.00504329614341259\n",
      "accuracy: 0.7459999322891235\n",
      "Epoch 34, CIFAR-10 Batch 1:  loss: 0.0031576540786772966\n",
      "accuracy: 0.7559998035430908\n",
      "Epoch 34, CIFAR-10 Batch 2:  loss: 0.010951285250484943\n",
      "accuracy: 0.7439998984336853\n",
      "Epoch 34, CIFAR-10 Batch 3:  loss: 0.011966951191425323\n",
      "accuracy: 0.7383998036384583\n",
      "Epoch 34, CIFAR-10 Batch 4:  loss: 0.005618441849946976\n",
      "accuracy: 0.7379998564720154\n",
      "Epoch 34, CIFAR-10 Batch 5:  loss: 0.005315764341503382\n",
      "accuracy: 0.7451997995376587\n",
      "Epoch 35, CIFAR-10 Batch 1:  loss: 0.001761366263963282\n",
      "accuracy: 0.7401998043060303\n",
      "Epoch 35, CIFAR-10 Batch 2:  loss: 0.0030077018309384584\n",
      "accuracy: 0.7395998239517212\n",
      "Epoch 35, CIFAR-10 Batch 3:  loss: 0.00637568486854434\n",
      "accuracy: 0.7503998279571533\n",
      "Epoch 35, CIFAR-10 Batch 4:  loss: 0.002892395481467247\n",
      "accuracy: 0.747799813747406\n",
      "Epoch 35, CIFAR-10 Batch 5:  loss: 0.0019766935147345066\n",
      "accuracy: 0.752599835395813\n",
      "Epoch 36, CIFAR-10 Batch 1:  loss: 0.003890071529895067\n",
      "accuracy: 0.7471998333930969\n",
      "Epoch 36, CIFAR-10 Batch 2:  loss: 0.0036202585324645042\n",
      "accuracy: 0.7399997711181641\n",
      "Epoch 36, CIFAR-10 Batch 3:  loss: 0.0013672480126842856\n",
      "accuracy: 0.7505998611450195\n",
      "Epoch 36, CIFAR-10 Batch 4:  loss: 0.005429399199783802\n",
      "accuracy: 0.7417998909950256\n",
      "Epoch 36, CIFAR-10 Batch 5:  loss: 0.0025272381026297808\n",
      "accuracy: 0.7381998896598816\n",
      "Epoch 37, CIFAR-10 Batch 1:  loss: 0.0034919683821499348\n",
      "accuracy: 0.7459998726844788\n",
      "Epoch 37, CIFAR-10 Batch 2:  loss: 0.005690474063158035\n",
      "accuracy: 0.7419998645782471\n",
      "Epoch 37, CIFAR-10 Batch 3:  loss: 0.0031971721909940243\n",
      "accuracy: 0.7367998361587524\n",
      "Epoch 37, CIFAR-10 Batch 4:  loss: 0.003775283694267273\n",
      "accuracy: 0.7487998604774475\n",
      "Epoch 37, CIFAR-10 Batch 5:  loss: 0.006056350190192461\n",
      "accuracy: 0.7437998056411743\n",
      "Epoch 38, CIFAR-10 Batch 1:  loss: 0.0008302503265440464\n",
      "accuracy: 0.7393998503684998\n",
      "Epoch 38, CIFAR-10 Batch 2:  loss: 0.0022171209566295147\n",
      "accuracy: 0.7399998307228088\n",
      "Epoch 38, CIFAR-10 Batch 3:  loss: 0.0014152135699987411\n",
      "accuracy: 0.7509998083114624\n",
      "Epoch 38, CIFAR-10 Batch 4:  loss: 0.001705983653664589\n",
      "accuracy: 0.7523998022079468\n",
      "Epoch 38, CIFAR-10 Batch 5:  loss: 0.0034343511797487736\n",
      "accuracy: 0.7589998245239258\n",
      "Epoch 39, CIFAR-10 Batch 1:  loss: 0.001040942152030766\n",
      "accuracy: 0.7505998015403748\n",
      "Epoch 39, CIFAR-10 Batch 2:  loss: 0.0012780515244230628\n",
      "accuracy: 0.7405998706817627\n",
      "Epoch 39, CIFAR-10 Batch 3:  loss: 0.0018799148965626955\n",
      "accuracy: 0.7487998008728027\n",
      "Epoch 39, CIFAR-10 Batch 4:  loss: 0.003990506753325462\n",
      "accuracy: 0.7461998462677002\n",
      "Epoch 39, CIFAR-10 Batch 5:  loss: 0.006923893932253122\n",
      "accuracy: 0.7503998279571533\n",
      "Epoch 40, CIFAR-10 Batch 1:  loss: 0.003911606501787901\n",
      "accuracy: 0.7435998320579529\n",
      "Epoch 40, CIFAR-10 Batch 2:  loss: 0.0013834144920110703\n",
      "accuracy: 0.749599814414978\n",
      "Epoch 40, CIFAR-10 Batch 3:  loss: 0.002072952687740326\n",
      "accuracy: 0.7519998550415039\n",
      "Epoch 40, CIFAR-10 Batch 4:  loss: 0.0011680151801556349\n",
      "accuracy: 0.7505998611450195\n",
      "Epoch 40, CIFAR-10 Batch 5:  loss: 0.0028525288216769695\n",
      "accuracy: 0.7491998672485352\n",
      "Epoch 41, CIFAR-10 Batch 1:  loss: 0.01884051226079464\n",
      "accuracy: 0.7481998205184937\n",
      "Epoch 41, CIFAR-10 Batch 2:  loss: 0.0010331830708310008\n",
      "accuracy: 0.7399998307228088\n",
      "Epoch 41, CIFAR-10 Batch 3:  loss: 0.004600030370056629\n",
      "accuracy: 0.7449998259544373\n",
      "Epoch 41, CIFAR-10 Batch 4:  loss: 0.001720331609249115\n",
      "accuracy: 0.7537998557090759\n",
      "Epoch 41, CIFAR-10 Batch 5:  loss: 0.0022486571688205004\n",
      "accuracy: 0.7519999146461487\n",
      "Epoch 42, CIFAR-10 Batch 1:  loss: 0.002246863441541791\n",
      "accuracy: 0.7549998760223389\n",
      "Epoch 42, CIFAR-10 Batch 2:  loss: 0.005666067823767662\n",
      "accuracy: 0.7431997656822205\n",
      "Epoch 42, CIFAR-10 Batch 3:  loss: 0.001299285446293652\n",
      "accuracy: 0.7505998611450195\n",
      "Epoch 42, CIFAR-10 Batch 4:  loss: 0.0023417617194354534\n",
      "accuracy: 0.7611998319625854\n",
      "Epoch 42, CIFAR-10 Batch 5:  loss: 0.0013427732046693563\n",
      "accuracy: 0.7527998685836792\n",
      "Epoch 43, CIFAR-10 Batch 1:  loss: 0.0010068968404084444\n",
      "accuracy: 0.7585998773574829\n",
      "Epoch 43, CIFAR-10 Batch 2:  loss: 0.0008771822322160006\n",
      "accuracy: 0.7499998211860657\n",
      "Epoch 43, CIFAR-10 Batch 3:  loss: 0.0029721390455961227\n",
      "accuracy: 0.7585997581481934\n",
      "Epoch 43, CIFAR-10 Batch 4:  loss: 0.00647452287375927\n",
      "accuracy: 0.7629998326301575\n",
      "Epoch 43, CIFAR-10 Batch 5:  loss: 0.0020871725864708424\n",
      "accuracy: 0.7507997751235962\n",
      "Epoch 44, CIFAR-10 Batch 1:  loss: 0.0007465558592230082\n",
      "accuracy: 0.7589998245239258\n",
      "Epoch 44, CIFAR-10 Batch 2:  loss: 0.004410352557897568\n",
      "accuracy: 0.7477998733520508\n",
      "Epoch 44, CIFAR-10 Batch 3:  loss: 0.004042025655508041\n",
      "accuracy: 0.7591997981071472\n",
      "Epoch 44, CIFAR-10 Batch 4:  loss: 0.0017212983220815659\n",
      "accuracy: 0.7629998922348022\n",
      "Epoch 44, CIFAR-10 Batch 5:  loss: 0.00202752323821187\n",
      "accuracy: 0.754399836063385\n",
      "Epoch 45, CIFAR-10 Batch 1:  loss: 0.0029136082157492638\n",
      "accuracy: 0.7531998753547668\n",
      "Epoch 45, CIFAR-10 Batch 2:  loss: 0.00992283970117569\n",
      "accuracy: 0.7469998598098755\n",
      "Epoch 45, CIFAR-10 Batch 3:  loss: 0.0030703977681696415\n",
      "accuracy: 0.7527998685836792\n",
      "Epoch 45, CIFAR-10 Batch 4:  loss: 0.0024819739628583193\n",
      "accuracy: 0.7525998950004578\n",
      "Epoch 45, CIFAR-10 Batch 5:  loss: 0.0021567256189882755\n",
      "accuracy: 0.7495998740196228\n",
      "Epoch 46, CIFAR-10 Batch 1:  loss: 0.0027932547964155674\n",
      "accuracy: 0.7601998448371887\n",
      "Epoch 46, CIFAR-10 Batch 2:  loss: 0.0006507080979645252\n",
      "accuracy: 0.7461997866630554\n",
      "Epoch 46, CIFAR-10 Batch 3:  loss: 0.0009452112717553973\n",
      "accuracy: 0.7585997581481934\n",
      "Epoch 46, CIFAR-10 Batch 4:  loss: 0.0026630365755409002\n",
      "accuracy: 0.7565998435020447\n",
      "Epoch 46, CIFAR-10 Batch 5:  loss: 0.003885792102664709\n",
      "accuracy: 0.7607998847961426\n",
      "Epoch 47, CIFAR-10 Batch 1:  loss: 0.0008212578832171857\n",
      "accuracy: 0.7441998720169067\n",
      "Epoch 47, CIFAR-10 Batch 2:  loss: 0.0030909618362784386\n",
      "accuracy: 0.7499998211860657\n",
      "Epoch 47, CIFAR-10 Batch 3:  loss: 0.00039197845035232604\n",
      "accuracy: 0.763999879360199\n",
      "Epoch 47, CIFAR-10 Batch 4:  loss: 0.007041541393846273\n",
      "accuracy: 0.7585998177528381\n",
      "Epoch 47, CIFAR-10 Batch 5:  loss: 0.0017130133928731084\n",
      "accuracy: 0.7671998739242554\n",
      "Epoch 48, CIFAR-10 Batch 1:  loss: 0.001722782733850181\n",
      "accuracy: 0.7537998557090759\n",
      "Epoch 48, CIFAR-10 Batch 2:  loss: 0.004274792969226837\n",
      "accuracy: 0.7425999045372009\n",
      "Epoch 48, CIFAR-10 Batch 3:  loss: 0.0006896182894706726\n",
      "accuracy: 0.7575998902320862\n",
      "Epoch 48, CIFAR-10 Batch 4:  loss: 0.0016756425611674786\n",
      "accuracy: 0.7555997967720032\n",
      "Epoch 48, CIFAR-10 Batch 5:  loss: 0.004730170592665672\n",
      "accuracy: 0.7537998557090759\n",
      "Epoch 49, CIFAR-10 Batch 1:  loss: 0.0013500937493517995\n",
      "accuracy: 0.7561997771263123\n",
      "Epoch 49, CIFAR-10 Batch 2:  loss: 0.0006586272502318025\n",
      "accuracy: 0.7295998334884644\n",
      "Epoch 49, CIFAR-10 Batch 3:  loss: 0.0024452656507492065\n",
      "accuracy: 0.7537997961044312\n",
      "Epoch 49, CIFAR-10 Batch 4:  loss: 0.0019166002748534083\n",
      "accuracy: 0.7553998231887817\n",
      "Epoch 49, CIFAR-10 Batch 5:  loss: 0.0014274684945121408\n",
      "accuracy: 0.7505998015403748\n",
      "Epoch 50, CIFAR-10 Batch 1:  loss: 0.0028414111584424973\n",
      "accuracy: 0.7603998184204102\n",
      "Epoch 50, CIFAR-10 Batch 2:  loss: 0.0006655994220636785\n",
      "accuracy: 0.7435998916625977\n",
      "Epoch 50, CIFAR-10 Batch 3:  loss: 0.00024997128639370203\n",
      "accuracy: 0.7585998773574829\n",
      "Epoch 50, CIFAR-10 Batch 4:  loss: 0.0015922999009490013\n",
      "accuracy: 0.7577998042106628\n",
      "Epoch 50, CIFAR-10 Batch 5:  loss: 0.0022150559816509485\n",
      "accuracy: 0.7531998157501221\n",
      "Epoch 51, CIFAR-10 Batch 1:  loss: 0.00038284805486910045\n",
      "accuracy: 0.7445998191833496\n",
      "Epoch 51, CIFAR-10 Batch 2:  loss: 0.001170651288703084\n",
      "accuracy: 0.7503998279571533\n",
      "Epoch 51, CIFAR-10 Batch 3:  loss: 0.002559780143201351\n",
      "accuracy: 0.7597998380661011\n",
      "Epoch 51, CIFAR-10 Batch 4:  loss: 0.002629037480801344\n",
      "accuracy: 0.7511998414993286\n",
      "Epoch 51, CIFAR-10 Batch 5:  loss: 0.0015264524845406413\n",
      "accuracy: 0.7535997629165649\n",
      "Epoch 52, CIFAR-10 Batch 1:  loss: 0.005014372058212757\n",
      "accuracy: 0.7487998008728027\n",
      "Epoch 52, CIFAR-10 Batch 2:  loss: 0.0009405325399711728\n",
      "accuracy: 0.754399836063385\n",
      "Epoch 52, CIFAR-10 Batch 3:  loss: 0.0006549475947394967\n",
      "accuracy: 0.7551997900009155\n",
      "Epoch 52, CIFAR-10 Batch 4:  loss: 0.003420979715883732\n",
      "accuracy: 0.7469998598098755\n",
      "Epoch 52, CIFAR-10 Batch 5:  loss: 0.0018464417662471533\n",
      "accuracy: 0.7487998604774475\n",
      "Epoch 53, CIFAR-10 Batch 1:  loss: 0.0007174224010668695\n",
      "accuracy: 0.7409998774528503\n",
      "Epoch 53, CIFAR-10 Batch 2:  loss: 0.0012408870970830321\n",
      "accuracy: 0.7347998023033142\n",
      "Epoch 53, CIFAR-10 Batch 3:  loss: 0.0014226771891117096\n",
      "accuracy: 0.754399836063385\n",
      "Epoch 53, CIFAR-10 Batch 4:  loss: 0.0006787633174099028\n",
      "accuracy: 0.7599998116493225\n",
      "Epoch 53, CIFAR-10 Batch 5:  loss: 0.005711039062589407\n",
      "accuracy: 0.7605998516082764\n",
      "Epoch 54, CIFAR-10 Batch 1:  loss: 0.0005096444510854781\n",
      "accuracy: 0.7539997696876526\n",
      "Epoch 54, CIFAR-10 Batch 2:  loss: 0.0009116155561059713\n",
      "accuracy: 0.7457998394966125\n",
      "Epoch 54, CIFAR-10 Batch 3:  loss: 0.009021848440170288\n",
      "accuracy: 0.7493998408317566\n",
      "Epoch 54, CIFAR-10 Batch 4:  loss: 0.0049376399256289005\n",
      "accuracy: 0.7557998895645142\n",
      "Epoch 54, CIFAR-10 Batch 5:  loss: 0.0007721985457465053\n",
      "accuracy: 0.7539997696876526\n",
      "Epoch 55, CIFAR-10 Batch 1:  loss: 0.00024007298634387553\n",
      "accuracy: 0.7551997900009155\n",
      "Epoch 55, CIFAR-10 Batch 2:  loss: 0.001421152031980455\n",
      "accuracy: 0.7481997609138489\n",
      "Epoch 55, CIFAR-10 Batch 3:  loss: 0.0008054435020312667\n",
      "accuracy: 0.7521997690200806\n",
      "Epoch 55, CIFAR-10 Batch 4:  loss: 0.005332749802619219\n",
      "accuracy: 0.7633998394012451\n",
      "Epoch 55, CIFAR-10 Batch 5:  loss: 0.009600188583135605\n",
      "accuracy: 0.747799813747406\n",
      "Epoch 56, CIFAR-10 Batch 1:  loss: 0.0002632509858813137\n",
      "accuracy: 0.7613998055458069\n",
      "Epoch 56, CIFAR-10 Batch 2:  loss: 0.0006144330254755914\n",
      "accuracy: 0.7485998272895813\n",
      "Epoch 56, CIFAR-10 Batch 3:  loss: 0.001402346882969141\n",
      "accuracy: 0.7573997974395752\n",
      "Epoch 56, CIFAR-10 Batch 4:  loss: 0.0006602465873584151\n",
      "accuracy: 0.7647997736930847\n",
      "Epoch 56, CIFAR-10 Batch 5:  loss: 0.0006205036770552397\n",
      "accuracy: 0.7499998807907104\n",
      "Epoch 57, CIFAR-10 Batch 1:  loss: 0.0005016277427785099\n",
      "accuracy: 0.7501998543739319\n",
      "Epoch 57, CIFAR-10 Batch 2:  loss: 0.0011127295438200235\n",
      "accuracy: 0.748999834060669\n",
      "Epoch 57, CIFAR-10 Batch 3:  loss: 0.0026901301462203264\n",
      "accuracy: 0.7639997601509094\n",
      "Epoch 57, CIFAR-10 Batch 4:  loss: 0.0009615238523110747\n",
      "accuracy: 0.7557998299598694\n",
      "Epoch 57, CIFAR-10 Batch 5:  loss: 0.0015943896723911166\n",
      "accuracy: 0.7655998468399048\n",
      "Epoch 58, CIFAR-10 Batch 1:  loss: 0.00020393714657984674\n",
      "accuracy: 0.7541998028755188\n",
      "Epoch 58, CIFAR-10 Batch 2:  loss: 0.0003048853832297027\n",
      "accuracy: 0.75139981508255\n",
      "Epoch 58, CIFAR-10 Batch 3:  loss: 0.000985029968433082\n",
      "accuracy: 0.7669998407363892\n",
      "Epoch 58, CIFAR-10 Batch 4:  loss: 0.0002612000098451972\n",
      "accuracy: 0.7623997926712036\n",
      "Epoch 58, CIFAR-10 Batch 5:  loss: 0.00016386246716137975\n",
      "accuracy: 0.7523998022079468\n",
      "Epoch 59, CIFAR-10 Batch 1:  loss: 0.0005026130238547921\n",
      "accuracy: 0.7423998713493347\n",
      "Epoch 59, CIFAR-10 Batch 2:  loss: 0.002894751727581024\n",
      "accuracy: 0.7549998164176941\n",
      "Epoch 59, CIFAR-10 Batch 3:  loss: 0.0025726058520376682\n",
      "accuracy: 0.7615998387336731\n",
      "Epoch 59, CIFAR-10 Batch 4:  loss: 0.002296029357239604\n",
      "accuracy: 0.7557998299598694\n",
      "Epoch 59, CIFAR-10 Batch 5:  loss: 0.0009364840225316584\n",
      "accuracy: 0.7483998537063599\n",
      "Epoch 60, CIFAR-10 Batch 1:  loss: 0.00042737432522699237\n",
      "accuracy: 0.7517998218536377\n",
      "Epoch 60, CIFAR-10 Batch 2:  loss: 0.001582753611728549\n",
      "accuracy: 0.7519998550415039\n",
      "Epoch 60, CIFAR-10 Batch 3:  loss: 0.00022911527776159346\n",
      "accuracy: 0.7619998455047607\n",
      "Epoch 60, CIFAR-10 Batch 4:  loss: 0.0005985179450362921\n",
      "accuracy: 0.7589998245239258\n",
      "Epoch 60, CIFAR-10 Batch 5:  loss: 0.00042560839210636914\n",
      "accuracy: 0.7691998481750488\n",
      "Epoch 61, CIFAR-10 Batch 1:  loss: 0.0012390381889417768\n",
      "accuracy: 0.7453998327255249\n",
      "Epoch 61, CIFAR-10 Batch 2:  loss: 0.0012318408116698265\n",
      "accuracy: 0.7685998678207397\n",
      "Epoch 61, CIFAR-10 Batch 3:  loss: 0.0023072126787155867\n",
      "accuracy: 0.7585998773574829\n",
      "Epoch 61, CIFAR-10 Batch 4:  loss: 0.0003735522914212197\n",
      "accuracy: 0.7597998380661011\n",
      "Epoch 61, CIFAR-10 Batch 5:  loss: 0.0004610178875736892\n",
      "accuracy: 0.7595998048782349\n",
      "Epoch 62, CIFAR-10 Batch 1:  loss: 0.0003266683197580278\n",
      "accuracy: 0.738399863243103\n",
      "Epoch 62, CIFAR-10 Batch 2:  loss: 0.001543573453091085\n",
      "accuracy: 0.756199836730957\n",
      "Epoch 62, CIFAR-10 Batch 3:  loss: 0.0007433883147314191\n",
      "accuracy: 0.7587998509407043\n",
      "Epoch 62, CIFAR-10 Batch 4:  loss: 0.0008828004356473684\n",
      "accuracy: 0.7657998204231262\n",
      "Epoch 62, CIFAR-10 Batch 5:  loss: 0.001718201208859682\n",
      "accuracy: 0.7635998129844666\n",
      "Epoch 63, CIFAR-10 Batch 1:  loss: 0.000324689201079309\n",
      "accuracy: 0.7535999417304993\n",
      "Epoch 63, CIFAR-10 Batch 2:  loss: 0.00034959265030920506\n",
      "accuracy: 0.7539998888969421\n",
      "Epoch 63, CIFAR-10 Batch 3:  loss: 0.0016747150802984834\n",
      "accuracy: 0.7691998481750488\n",
      "Epoch 63, CIFAR-10 Batch 4:  loss: 0.00040298645035363734\n",
      "accuracy: 0.7675997614860535\n",
      "Epoch 63, CIFAR-10 Batch 5:  loss: 0.00025709846522659063\n",
      "accuracy: 0.752599835395813\n",
      "Epoch 64, CIFAR-10 Batch 1:  loss: 0.0006663000676780939\n",
      "accuracy: 0.7475998401641846\n",
      "Epoch 64, CIFAR-10 Batch 2:  loss: 0.00025526422541588545\n",
      "accuracy: 0.7581998705863953\n",
      "Epoch 64, CIFAR-10 Batch 3:  loss: 0.0002685007930267602\n",
      "accuracy: 0.7647998929023743\n",
      "Epoch 64, CIFAR-10 Batch 4:  loss: 0.0013273522490635514\n",
      "accuracy: 0.7617998123168945\n",
      "Epoch 64, CIFAR-10 Batch 5:  loss: 0.0009334159549325705\n",
      "accuracy: 0.7569999098777771\n",
      "Epoch 65, CIFAR-10 Batch 1:  loss: 0.00039552731323055923\n",
      "accuracy: 0.7607998251914978\n",
      "Epoch 65, CIFAR-10 Batch 2:  loss: 0.00020745344227179885\n",
      "accuracy: 0.751599907875061\n",
      "Epoch 65, CIFAR-10 Batch 3:  loss: 0.0002606286434456706\n",
      "accuracy: 0.7641998529434204\n",
      "Epoch 65, CIFAR-10 Batch 4:  loss: 0.0009771903278306127\n",
      "accuracy: 0.7641998529434204\n",
      "Epoch 65, CIFAR-10 Batch 5:  loss: 0.00045475823571905494\n",
      "accuracy: 0.7609997987747192\n",
      "Epoch 66, CIFAR-10 Batch 1:  loss: 0.00035395249142311513\n",
      "accuracy: 0.7483998537063599\n",
      "Epoch 66, CIFAR-10 Batch 2:  loss: 0.0020593074150383472\n",
      "accuracy: 0.7547999024391174\n",
      "Epoch 66, CIFAR-10 Batch 3:  loss: 0.00037900256575085223\n",
      "accuracy: 0.7647997736930847\n",
      "Epoch 66, CIFAR-10 Batch 4:  loss: 0.0008079098188318312\n",
      "accuracy: 0.7643998861312866\n",
      "Epoch 66, CIFAR-10 Batch 5:  loss: 0.0043550701811909676\n",
      "accuracy: 0.7489997744560242\n",
      "Epoch 67, CIFAR-10 Batch 1:  loss: 0.00011984525917796418\n",
      "accuracy: 0.7647998332977295\n",
      "Epoch 67, CIFAR-10 Batch 2:  loss: 0.0006581278867088258\n",
      "accuracy: 0.744999885559082\n",
      "Epoch 67, CIFAR-10 Batch 3:  loss: 0.00023565127048641443\n",
      "accuracy: 0.7657998204231262\n",
      "Epoch 67, CIFAR-10 Batch 4:  loss: 0.0005456125945784152\n",
      "accuracy: 0.7619998455047607\n",
      "Epoch 67, CIFAR-10 Batch 5:  loss: 0.00029962073313072324\n",
      "accuracy: 0.7659998536109924\n",
      "Epoch 68, CIFAR-10 Batch 1:  loss: 6.325807771645486e-05\n",
      "accuracy: 0.7551998496055603\n",
      "Epoch 68, CIFAR-10 Batch 2:  loss: 0.0008146845502778888\n",
      "accuracy: 0.7469998598098755\n",
      "Epoch 68, CIFAR-10 Batch 3:  loss: 7.352916145464405e-05\n",
      "accuracy: 0.7607998251914978\n",
      "Epoch 68, CIFAR-10 Batch 4:  loss: 0.0009768296731635928\n",
      "accuracy: 0.7565998435020447\n",
      "Epoch 68, CIFAR-10 Batch 5:  loss: 0.0006268123397603631\n",
      "accuracy: 0.7583997845649719\n",
      "Epoch 69, CIFAR-10 Batch 1:  loss: 0.0004881530476268381\n",
      "accuracy: 0.745999813079834\n",
      "Epoch 69, CIFAR-10 Batch 2:  loss: 0.02663775160908699\n",
      "accuracy: 0.7549998760223389\n",
      "Epoch 69, CIFAR-10 Batch 3:  loss: 0.001468653092160821\n",
      "accuracy: 0.7619998455047607\n",
      "Epoch 69, CIFAR-10 Batch 4:  loss: 0.0001641609997022897\n",
      "accuracy: 0.7671998143196106\n",
      "Epoch 69, CIFAR-10 Batch 5:  loss: 0.004120782483369112\n",
      "accuracy: 0.7613998651504517\n",
      "Epoch 70, CIFAR-10 Batch 1:  loss: 0.002351578325033188\n",
      "accuracy: 0.7519997954368591\n",
      "Epoch 70, CIFAR-10 Batch 2:  loss: 0.012087036855518818\n",
      "accuracy: 0.7541999220848083\n",
      "Epoch 70, CIFAR-10 Batch 3:  loss: 0.0013713592197746038\n",
      "accuracy: 0.7641998529434204\n",
      "Epoch 70, CIFAR-10 Batch 4:  loss: 0.0005739678163081408\n",
      "accuracy: 0.7607998251914978\n",
      "Epoch 70, CIFAR-10 Batch 5:  loss: 0.0009459230932407081\n",
      "accuracy: 0.7659997940063477\n",
      "Epoch 71, CIFAR-10 Batch 1:  loss: 0.0009020938887260854\n",
      "accuracy: 0.755599856376648\n",
      "Epoch 71, CIFAR-10 Batch 2:  loss: 0.0014963796129450202\n",
      "accuracy: 0.7537997961044312\n",
      "Epoch 71, CIFAR-10 Batch 3:  loss: 0.0002762265794444829\n",
      "accuracy: 0.7631998062133789\n",
      "Epoch 71, CIFAR-10 Batch 4:  loss: 0.001450410345569253\n",
      "accuracy: 0.7575998306274414\n",
      "Epoch 71, CIFAR-10 Batch 5:  loss: 0.0013563315151259303\n",
      "accuracy: 0.75739985704422\n",
      "Epoch 72, CIFAR-10 Batch 1:  loss: 0.00018823560094460845\n",
      "accuracy: 0.7629998326301575\n",
      "Epoch 72, CIFAR-10 Batch 2:  loss: 0.0003191657306160778\n",
      "accuracy: 0.7461998462677002\n",
      "Epoch 72, CIFAR-10 Batch 3:  loss: 0.0005799322389066219\n",
      "accuracy: 0.7623997926712036\n",
      "Epoch 72, CIFAR-10 Batch 4:  loss: 0.015865923836827278\n",
      "accuracy: 0.7509998679161072\n",
      "Epoch 72, CIFAR-10 Batch 5:  loss: 0.0008117109537124634\n",
      "accuracy: 0.765799880027771\n",
      "Epoch 73, CIFAR-10 Batch 1:  loss: 0.0002876577200368047\n",
      "accuracy: 0.7631998062133789\n",
      "Epoch 73, CIFAR-10 Batch 2:  loss: 0.0005905568250454962\n",
      "accuracy: 0.7513998746871948\n",
      "Epoch 73, CIFAR-10 Batch 3:  loss: 8.18578846519813e-05\n",
      "accuracy: 0.765799880027771\n",
      "Epoch 73, CIFAR-10 Batch 4:  loss: 0.0002892460906878114\n",
      "accuracy: 0.7617998123168945\n",
      "Epoch 73, CIFAR-10 Batch 5:  loss: 0.0010444989893585443\n",
      "accuracy: 0.7623998522758484\n",
      "Epoch 74, CIFAR-10 Batch 1:  loss: 0.0004099676152691245\n",
      "accuracy: 0.7611998319625854\n",
      "Epoch 74, CIFAR-10 Batch 2:  loss: 0.00021659610501956195\n",
      "accuracy: 0.757999837398529\n",
      "Epoch 74, CIFAR-10 Batch 3:  loss: 0.00040385284228250384\n",
      "accuracy: 0.7535998225212097\n",
      "Epoch 74, CIFAR-10 Batch 4:  loss: 0.0004214738146401942\n",
      "accuracy: 0.7677998542785645\n",
      "Epoch 74, CIFAR-10 Batch 5:  loss: 0.0008789235143922269\n",
      "accuracy: 0.7693998217582703\n",
      "Epoch 75, CIFAR-10 Batch 1:  loss: 0.00015166420780587941\n",
      "accuracy: 0.76559978723526\n",
      "Epoch 75, CIFAR-10 Batch 2:  loss: 0.006837266962975264\n",
      "accuracy: 0.7635997533798218\n",
      "Epoch 75, CIFAR-10 Batch 3:  loss: 0.00030069847707636654\n",
      "accuracy: 0.7597998976707458\n",
      "Epoch 75, CIFAR-10 Batch 4:  loss: 0.00017191369261126965\n",
      "accuracy: 0.7575998306274414\n",
      "Epoch 75, CIFAR-10 Batch 5:  loss: 0.0005718094180338085\n",
      "accuracy: 0.7635998725891113\n",
      "Epoch 76, CIFAR-10 Batch 1:  loss: 5.404110561357811e-05\n",
      "accuracy: 0.7707997560501099\n",
      "Epoch 76, CIFAR-10 Batch 2:  loss: 0.00109287875238806\n",
      "accuracy: 0.7493997812271118\n",
      "Epoch 76, CIFAR-10 Batch 3:  loss: 0.00016866595251485705\n",
      "accuracy: 0.7659998536109924\n",
      "Epoch 76, CIFAR-10 Batch 4:  loss: 0.0010969308204948902\n",
      "accuracy: 0.7613998651504517\n",
      "Epoch 76, CIFAR-10 Batch 5:  loss: 0.00019535476167220622\n",
      "accuracy: 0.7671998143196106\n",
      "Epoch 77, CIFAR-10 Batch 1:  loss: 0.0015725567936897278\n",
      "accuracy: 0.7573997974395752\n",
      "Epoch 77, CIFAR-10 Batch 2:  loss: 0.0030714974272996187\n",
      "accuracy: 0.7551997900009155\n",
      "Epoch 77, CIFAR-10 Batch 3:  loss: 0.0002892398915719241\n",
      "accuracy: 0.7649998664855957\n",
      "Epoch 77, CIFAR-10 Batch 4:  loss: 0.000353874231223017\n",
      "accuracy: 0.7539998292922974\n",
      "Epoch 77, CIFAR-10 Batch 5:  loss: 0.0018628553953021765\n",
      "accuracy: 0.767399787902832\n",
      "Epoch 78, CIFAR-10 Batch 1:  loss: 0.0004166386788710952\n",
      "accuracy: 0.7689998745918274\n",
      "Epoch 78, CIFAR-10 Batch 2:  loss: 0.0006189603009261191\n",
      "accuracy: 0.7525998950004578\n",
      "Epoch 78, CIFAR-10 Batch 3:  loss: 0.00020438747014850378\n",
      "accuracy: 0.7629998922348022\n",
      "Epoch 78, CIFAR-10 Batch 4:  loss: 0.00022526818793267012\n",
      "accuracy: 0.7611998319625854\n",
      "Epoch 78, CIFAR-10 Batch 5:  loss: 0.0008460112730972469\n",
      "accuracy: 0.7643998265266418\n",
      "Epoch 79, CIFAR-10 Batch 1:  loss: 0.00040327172609977424\n",
      "accuracy: 0.7647998332977295\n",
      "Epoch 79, CIFAR-10 Batch 2:  loss: 0.002642640843987465\n",
      "accuracy: 0.7701998353004456\n",
      "Epoch 79, CIFAR-10 Batch 3:  loss: 0.00021888325863983482\n",
      "accuracy: 0.7647998929023743\n",
      "Epoch 79, CIFAR-10 Batch 4:  loss: 7.158452353905886e-05\n",
      "accuracy: 0.7591997385025024\n",
      "Epoch 79, CIFAR-10 Batch 5:  loss: 0.0010794838890433311\n",
      "accuracy: 0.7595998048782349\n",
      "Epoch 80, CIFAR-10 Batch 1:  loss: 0.00014784629456698895\n",
      "accuracy: 0.7637998461723328\n",
      "Epoch 80, CIFAR-10 Batch 2:  loss: 0.0005948594771325588\n",
      "accuracy: 0.7653998732566833\n",
      "Epoch 80, CIFAR-10 Batch 3:  loss: 0.0005447309813462198\n",
      "accuracy: 0.7633998394012451\n",
      "Epoch 80, CIFAR-10 Batch 4:  loss: 0.00017310184193775058\n",
      "accuracy: 0.7689998149871826\n",
      "Epoch 80, CIFAR-10 Batch 5:  loss: 0.0002701914054341614\n",
      "accuracy: 0.770599901676178\n",
      "Epoch 81, CIFAR-10 Batch 1:  loss: 0.000685475766658783\n",
      "accuracy: 0.7691998481750488\n",
      "Epoch 81, CIFAR-10 Batch 2:  loss: 0.0003811974311247468\n",
      "accuracy: 0.754399836063385\n",
      "Epoch 81, CIFAR-10 Batch 3:  loss: 0.00014908536104485393\n",
      "accuracy: 0.7615998387336731\n",
      "Epoch 81, CIFAR-10 Batch 4:  loss: 0.00046666120761074126\n",
      "accuracy: 0.7569998502731323\n",
      "Epoch 81, CIFAR-10 Batch 5:  loss: 8.820135553833097e-05\n",
      "accuracy: 0.7649998068809509\n",
      "Epoch 82, CIFAR-10 Batch 1:  loss: 0.001047335797920823\n",
      "accuracy: 0.767599880695343\n",
      "Epoch 82, CIFAR-10 Batch 2:  loss: 0.00010160724923480302\n",
      "accuracy: 0.7665998339653015\n",
      "Epoch 82, CIFAR-10 Batch 3:  loss: 0.00028006508364342153\n",
      "accuracy: 0.7611998319625854\n",
      "Epoch 82, CIFAR-10 Batch 4:  loss: 3.712158286361955e-05\n",
      "accuracy: 0.7629998326301575\n",
      "Epoch 82, CIFAR-10 Batch 5:  loss: 0.00019130526925437152\n",
      "accuracy: 0.7655998468399048\n",
      "Epoch 83, CIFAR-10 Batch 1:  loss: 0.00017361954087391496\n",
      "accuracy: 0.7613998651504517\n",
      "Epoch 83, CIFAR-10 Batch 2:  loss: 0.0007267965120263398\n",
      "accuracy: 0.7609997987747192\n",
      "Epoch 83, CIFAR-10 Batch 3:  loss: 0.0012750244932249188\n",
      "accuracy: 0.7695998549461365\n",
      "Epoch 83, CIFAR-10 Batch 4:  loss: 0.0003008510684594512\n",
      "accuracy: 0.7653998136520386\n",
      "Epoch 83, CIFAR-10 Batch 5:  loss: 0.00022157722560223192\n",
      "accuracy: 0.7639998197555542\n",
      "Epoch 84, CIFAR-10 Batch 1:  loss: 0.0006322142435237765\n",
      "accuracy: 0.7549998164176941\n",
      "Epoch 84, CIFAR-10 Batch 2:  loss: 0.00037945355870760977\n",
      "accuracy: 0.7695997953414917\n",
      "Epoch 84, CIFAR-10 Batch 3:  loss: 0.0003681473317556083\n",
      "accuracy: 0.7627997994422913\n",
      "Epoch 84, CIFAR-10 Batch 4:  loss: 0.0011578031117096543\n",
      "accuracy: 0.7689998149871826\n",
      "Epoch 84, CIFAR-10 Batch 5:  loss: 0.00268014264293015\n",
      "accuracy: 0.7607998251914978\n",
      "Epoch 85, CIFAR-10 Batch 1:  loss: 0.00010452679998707026\n",
      "accuracy: 0.7615997791290283\n",
      "Epoch 85, CIFAR-10 Batch 2:  loss: 0.00055260508088395\n",
      "accuracy: 0.7645998597145081\n",
      "Epoch 85, CIFAR-10 Batch 3:  loss: 0.0004418334283400327\n",
      "accuracy: 0.7649998068809509\n",
      "Epoch 85, CIFAR-10 Batch 4:  loss: 0.000426723447162658\n",
      "accuracy: 0.7667998671531677\n",
      "Epoch 85, CIFAR-10 Batch 5:  loss: 0.00031383399618789554\n",
      "accuracy: 0.7565998435020447\n",
      "Epoch 86, CIFAR-10 Batch 1:  loss: 0.00033670401899144053\n",
      "accuracy: 0.7649998664855957\n",
      "Epoch 86, CIFAR-10 Batch 2:  loss: 0.00035405263770371675\n",
      "accuracy: 0.7677997350692749\n",
      "Epoch 86, CIFAR-10 Batch 3:  loss: 0.001097442931495607\n",
      "accuracy: 0.7669998407363892\n",
      "Epoch 86, CIFAR-10 Batch 4:  loss: 0.00016914373554755002\n",
      "accuracy: 0.7623998522758484\n",
      "Epoch 86, CIFAR-10 Batch 5:  loss: 0.00023001835506875068\n",
      "accuracy: 0.7611998319625854\n",
      "Epoch 87, CIFAR-10 Batch 1:  loss: 0.005822345614433289\n",
      "accuracy: 0.7647998332977295\n",
      "Epoch 87, CIFAR-10 Batch 2:  loss: 0.0004695910611189902\n",
      "accuracy: 0.7651997804641724\n",
      "Epoch 87, CIFAR-10 Batch 3:  loss: 0.00048262206837534904\n",
      "accuracy: 0.7597998380661011\n",
      "Epoch 87, CIFAR-10 Batch 4:  loss: 0.00013075802417006344\n",
      "accuracy: 0.7617998123168945\n",
      "Epoch 87, CIFAR-10 Batch 5:  loss: 0.004324642941355705\n",
      "accuracy: 0.7593998312950134\n",
      "Epoch 88, CIFAR-10 Batch 1:  loss: 8.938623795984313e-05\n",
      "accuracy: 0.7591997385025024\n",
      "Epoch 88, CIFAR-10 Batch 2:  loss: 0.0009893248789012432\n",
      "accuracy: 0.7607998847961426\n",
      "Epoch 88, CIFAR-10 Batch 3:  loss: 0.00026484846603125334\n",
      "accuracy: 0.7679998278617859\n",
      "Epoch 88, CIFAR-10 Batch 4:  loss: 0.00012665314716286957\n",
      "accuracy: 0.7643997669219971\n",
      "Epoch 88, CIFAR-10 Batch 5:  loss: 0.0034836456179618835\n",
      "accuracy: 0.7595998644828796\n",
      "Epoch 89, CIFAR-10 Batch 1:  loss: 0.0002751346037257463\n",
      "accuracy: 0.768599808216095\n",
      "Epoch 89, CIFAR-10 Batch 2:  loss: 0.0008845856063999236\n",
      "accuracy: 0.7683998346328735\n",
      "Epoch 89, CIFAR-10 Batch 3:  loss: 7.028353866189718e-05\n",
      "accuracy: 0.7569997906684875\n",
      "Epoch 89, CIFAR-10 Batch 4:  loss: 0.000607186695560813\n",
      "accuracy: 0.7647998332977295\n",
      "Epoch 89, CIFAR-10 Batch 5:  loss: 0.0013184745330363512\n",
      "accuracy: 0.7581997513771057\n",
      "Epoch 90, CIFAR-10 Batch 1:  loss: 0.0002105495223077014\n",
      "accuracy: 0.7657998204231262\n",
      "Epoch 90, CIFAR-10 Batch 2:  loss: 0.000443737197201699\n",
      "accuracy: 0.7671998739242554\n",
      "Epoch 90, CIFAR-10 Batch 3:  loss: 0.00037478204467333853\n",
      "accuracy: 0.7595998048782349\n",
      "Epoch 90, CIFAR-10 Batch 4:  loss: 0.0006437248084694147\n",
      "accuracy: 0.7541998624801636\n",
      "Epoch 90, CIFAR-10 Batch 5:  loss: 0.00014009671576786786\n",
      "accuracy: 0.7657998204231262\n",
      "Epoch 91, CIFAR-10 Batch 1:  loss: 0.0002364714164286852\n",
      "accuracy: 0.7619998455047607\n",
      "Epoch 91, CIFAR-10 Batch 2:  loss: 8.715925650903955e-05\n",
      "accuracy: 0.7679998278617859\n",
      "Epoch 91, CIFAR-10 Batch 3:  loss: 0.0003900801530107856\n",
      "accuracy: 0.7623998522758484\n",
      "Epoch 91, CIFAR-10 Batch 4:  loss: 0.00011651813838398084\n",
      "accuracy: 0.7611998319625854\n",
      "Epoch 91, CIFAR-10 Batch 5:  loss: 0.0011782811488956213\n",
      "accuracy: 0.7705998420715332\n",
      "Epoch 92, CIFAR-10 Batch 1:  loss: 0.0006665971595793962\n",
      "accuracy: 0.7613998055458069\n",
      "Epoch 92, CIFAR-10 Batch 2:  loss: 0.001620761351659894\n",
      "accuracy: 0.7597997784614563\n",
      "Epoch 92, CIFAR-10 Batch 3:  loss: 0.00010354456026107073\n",
      "accuracy: 0.7675999402999878\n",
      "Epoch 92, CIFAR-10 Batch 4:  loss: 0.0002992466324940324\n",
      "accuracy: 0.7587998509407043\n",
      "Epoch 92, CIFAR-10 Batch 5:  loss: 0.00016146850248333067\n",
      "accuracy: 0.7671998739242554\n",
      "Epoch 93, CIFAR-10 Batch 1:  loss: 0.000588917697314173\n",
      "accuracy: 0.7647998929023743\n",
      "Epoch 93, CIFAR-10 Batch 2:  loss: 0.00023663121100980788\n",
      "accuracy: 0.7597998380661011\n",
      "Epoch 93, CIFAR-10 Batch 3:  loss: 8.608893404016271e-05\n",
      "accuracy: 0.762799859046936\n",
      "Epoch 93, CIFAR-10 Batch 4:  loss: 0.00013115257024765015\n",
      "accuracy: 0.7535998821258545\n",
      "Epoch 93, CIFAR-10 Batch 5:  loss: 0.0038503282703459263\n",
      "accuracy: 0.7583998441696167\n",
      "Epoch 94, CIFAR-10 Batch 1:  loss: 0.0016086245886981487\n",
      "accuracy: 0.7551997900009155\n",
      "Epoch 94, CIFAR-10 Batch 2:  loss: 0.0004764764744322747\n",
      "accuracy: 0.7609997987747192\n",
      "Epoch 94, CIFAR-10 Batch 3:  loss: 0.00021332378673832864\n",
      "accuracy: 0.7657998204231262\n",
      "Epoch 94, CIFAR-10 Batch 4:  loss: 0.0010493934387341142\n",
      "accuracy: 0.7579997777938843\n",
      "Epoch 94, CIFAR-10 Batch 5:  loss: 0.00020102012786082923\n",
      "accuracy: 0.7583998441696167\n",
      "Epoch 95, CIFAR-10 Batch 1:  loss: 0.00020591015345416963\n",
      "accuracy: 0.7607998847961426\n",
      "Epoch 95, CIFAR-10 Batch 2:  loss: 0.011512069031596184\n",
      "accuracy: 0.755599856376648\n",
      "Epoch 95, CIFAR-10 Batch 3:  loss: 0.00016753881936892867\n",
      "accuracy: 0.7499998211860657\n",
      "Epoch 95, CIFAR-10 Batch 4:  loss: 8.216699643526226e-05\n",
      "accuracy: 0.7645997405052185\n",
      "Epoch 95, CIFAR-10 Batch 5:  loss: 5.860200690221973e-05\n",
      "accuracy: 0.7659997940063477\n",
      "Epoch 96, CIFAR-10 Batch 1:  loss: 0.0009649489657022059\n",
      "accuracy: 0.7613998651504517\n",
      "Epoch 96, CIFAR-10 Batch 2:  loss: 0.0013342015445232391\n",
      "accuracy: 0.7635998725891113\n",
      "Epoch 96, CIFAR-10 Batch 3:  loss: 3.757299418793991e-05\n",
      "accuracy: 0.7639998197555542\n",
      "Epoch 96, CIFAR-10 Batch 4:  loss: 0.00014416391786653548\n",
      "accuracy: 0.757199764251709\n",
      "Epoch 96, CIFAR-10 Batch 5:  loss: 0.0001598087401362136\n",
      "accuracy: 0.7641998529434204\n",
      "Epoch 97, CIFAR-10 Batch 1:  loss: 6.181564094731584e-05\n",
      "accuracy: 0.7611997723579407\n",
      "Epoch 97, CIFAR-10 Batch 2:  loss: 0.00015270063886418939\n",
      "accuracy: 0.7673998475074768\n",
      "Epoch 97, CIFAR-10 Batch 3:  loss: 0.0001221999409608543\n",
      "accuracy: 0.7643998265266418\n",
      "Epoch 97, CIFAR-10 Batch 4:  loss: 0.0001295275433221832\n",
      "accuracy: 0.7611998319625854\n",
      "Epoch 97, CIFAR-10 Batch 5:  loss: 3.405895768082701e-05\n",
      "accuracy: 0.7645998597145081\n",
      "Epoch 98, CIFAR-10 Batch 1:  loss: 9.798961400520056e-05\n",
      "accuracy: 0.7571998834609985\n",
      "Epoch 98, CIFAR-10 Batch 2:  loss: 0.0006810547201894224\n",
      "accuracy: 0.7675998210906982\n",
      "Epoch 98, CIFAR-10 Batch 3:  loss: 0.002333811018615961\n",
      "accuracy: 0.7523998022079468\n",
      "Epoch 98, CIFAR-10 Batch 4:  loss: 0.00016927272372413427\n",
      "accuracy: 0.7597997784614563\n",
      "Epoch 98, CIFAR-10 Batch 5:  loss: 4.7734407417010516e-05\n",
      "accuracy: 0.7583998441696167\n",
      "Epoch 99, CIFAR-10 Batch 1:  loss: 9.899691940518096e-05\n",
      "accuracy: 0.7679998278617859\n",
      "Epoch 99, CIFAR-10 Batch 2:  loss: 0.0028075873851776123\n",
      "accuracy: 0.7643998861312866\n",
      "Epoch 99, CIFAR-10 Batch 3:  loss: 0.00012730431626550853\n",
      "accuracy: 0.766799807548523\n",
      "Epoch 99, CIFAR-10 Batch 4:  loss: 0.00018234478193335235\n",
      "accuracy: 0.7709997892379761\n",
      "Epoch 99, CIFAR-10 Batch 5:  loss: 0.0003995892184320837\n",
      "accuracy: 0.7615997791290283\n",
      "Epoch 100, CIFAR-10 Batch 1:  loss: 0.0001268574269488454\n",
      "accuracy: 0.7617998719215393\n",
      "Epoch 100, CIFAR-10 Batch 2:  loss: 0.0015498934080824256\n",
      "accuracy: 0.7597998380661011\n",
      "Epoch 100, CIFAR-10 Batch 3:  loss: 5.3320058214012533e-05\n",
      "accuracy: 0.7647998332977295\n",
      "Epoch 100, CIFAR-10 Batch 4:  loss: 0.00025753973750397563\n",
      "accuracy: 0.7649998664855957\n",
      "Epoch 100, CIFAR-10 Batch 5:  loss: 3.015100082848221e-05\n",
      "accuracy: 0.7639998197555542\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7574169303797469\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAArsAAAJ/CAYAAAB4GhsgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f6338f78f60>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 349
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
